{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.4"
    },
    "colab": {
      "name": "Customer Lifetime Value.ipynb",
      "provenance": [],
      "include_colab_link": true
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/AbderrhmanAbdellatif/CRM/blob/master/Customer%20Lifetime%20Value.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0LJofSzuBdhe",
        "colab_type": "text"
      },
      "source": [
        "### Calculating Lifetime Value is the easy part. First we need to select a time window. It can be anything like 3, 6, 12, 24 months. By the equation below, we can have Lifetime Value for each customer in that specific time window:"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pnks6Q3KBdhk",
        "colab_type": "text"
      },
      "source": [
        "### Lifetime Value: Total Gross Revenue - Total Cost    >> ne demek istemis , Total Gross Revenue nedir ,   Total Cost nedir"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eWtDurXCBdhp",
        "colab_type": "text"
      },
      "source": [
        "### This equation now gives us the historical lifetime value. If we see some customers having very high negative lifetime value historically, it could be too late to take an action. At this point, we need to predict the future with machine learning:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "600VBmbkBdh2",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#import libraries\n",
        "from datetime import datetime, timedelta,date\n",
        "import pandas as pd\n",
        "%matplotlib inline\n",
        "from sklearn.metrics import classification_report,confusion_matrix\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "import seaborn as sns\n",
        "from sklearn.cluster import KMeans\n",
        "#import machine learning related libraries\n",
        "from sklearn.svm import SVC \n",
        "from sklearn.multioutput import MultiOutputClassifier \n",
        "from sklearn.ensemble import GradientBoostingClassifier \n",
        "from sklearn.tree import DecisionTreeClassifier \n",
        "from sklearn.neighbors import KNeighborsClassifier \n",
        "from sklearn.naive_bayes import GaussianNB \n",
        "from sklearn.ensemble import RandomForestClassifier \n",
        "from sklearn.linear_model import LogisticRegression\n",
        "import xgboost as xgb \n",
        "from sklearn.model_selection import KFold, cross_val_score, train_test_split\n",
        "import plotly.offline as pyoff\n",
        "import plotly.graph_objs as go\n",
        "import xgboost as xgb\n",
        "import datetime as dt\n",
        "from sklearn.model_selection import KFold, cross_val_score, train_test_split\n",
        "\n",
        "import xgboost as xgb\n",
        "\n",
        "#initate plotly\n",
        "pyoff.init_notebook_mode()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zN43obiRBdiK",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#read data from csv and redo the data work we done before\n",
        "tx_data = pd.read_csv('https://raw.githubusercontent.com/AbderrhmanAbdellatif/CRM/master/OnlineRetail.csv',encoding= 'unicode_escape')\n",
        "tx_data['InvoiceDate'] = pd.to_datetime(tx_data['InvoiceDate'])\n",
        "tx_uk = tx_data.query(\"Country=='United Kingdom'\").reset_index(drop=True)\n",
        "tx_uk.head()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2WNEJY6zdRk2",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "filtered_data=tx_data[['Country','CustomerID']].drop_duplicates()\n",
        "filtered_data.Country.value_counts()[:10].plot(kind='bar')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vXKghESadmj5",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "tx_data.describe()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aQVtqfG-fH1l",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "uk_data = tx_data.describe()[(tx_data.describe()['Quantity']>0)]\n",
        "uk_data=tx_data[['CustomerID','InvoiceDate','InvoiceNo','Quantity','UnitPrice']]\n",
        "uk_data['TotalPrice'] = uk_data['Quantity'] * uk_data['UnitPrice']\n",
        "uk_data['InvoiceDate'].min(),uk_data['InvoiceDate'].max()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SWiVcrWzfdgN",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "PRESENT = dt.datetime(2011,12,10)\n",
        "uk_data['InvoiceDate'] = pd.to_datetime(uk_data['InvoiceDate'])\n",
        "uk_data.head()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "48dNm1LSfa3M",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "rfm= uk_data.groupby('CustomerID').agg({'InvoiceDate': lambda date: (PRESENT - date.max()).days,\n",
        "                                        'InvoiceNo': lambda num: len(num),\n",
        "                                        'TotalPrice': lambda price: price.sum()})"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TXgAONOwnynB",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "rfm.columns"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_F0gdQspn70A",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "rfm.columns=['recency','frequency','monetary']\n",
        "rfm['recency'] = rfm['recency'].astype(int)\n",
        "rfm"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PR9RUZfZjbeY",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "wcss = {}\n",
        "for k in range(1,11):#30 adet orenklerememiz oldugu icin 1'den 30'a kadar gitmesini sagliyoruz\n",
        "    kmeans = KMeans(n_clusters= k, init= 'k-means++', max_iter= 300)\n",
        "    kmeans.fit(rfm)# standartlastirdigimi datamiza gore fit et, yani modelimizi olustur.\n",
        "    wcss[k] = kmeans.inertia_#inertia ne demek ? her bir k degeri icin wcss degerini bul  \n",
        "sns.pointplot(x = list(wcss.keys()), y = list(wcss.values()))\n",
        "wcss"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-lwysL0qRbJb",
        "colab_type": "text"
      },
      "source": [
        "\n",
        "Müşterilerimizi segmentlere ayırdık ve kimin en iyisi olduğunu öğrendik. Şimdi yakından izlememiz gereken en önemli metriklerden birini ölçme zamanı: \n",
        "**Müşteri Yaşam Boyu Değeri**\n",
        "\n",
        "Gelir elde etmek ve kârlı olmak için müşterilere (edinme maliyetleri, çevrimdışı reklamlar, promosyonlar, indirimler vb.) Yatırım yapıyoruz. Doğal olarak, bu eylemler bazı müşterileri yaşam boyu değer açısından süper değerli kılar, ancak her zaman karlılığı azaltan bazı müşteriler vardır. Bu davranış kalıplarını tanımlamamız, müşterileri segmentlere ayırmamız ve buna göre hareket etmemiz gerekiyor.\n",
        "\n",
        "\n",
        "Yaşam Boyu Değerin hesaplanması kolay bir parçadır. İlk önce bir zaman penceresi seçmeliyiz. 3, 6, 12, 24 ay gibi bir şey olabilir. Aşağıdaki denklemle, belirli bir zaman penceresinde her müşteri için Yaşam Boyu Değere sahip olabiliriz:\n",
        "\n",
        "\n",
        "**Ömür Boyu Değer**: Toplam Brüt Gelir - Toplam Maliyet\n",
        "\n",
        "Bu denklem şimdi bize tarihsel yaşam boyu değer veriyor. Tarihsel olarak çok yüksek negatif yaşam boyu değere sahip bazı müşteriler görürsek, harekete geçmek için çok geç olabilir. Bu noktada, makine öğrenimi ile geleceği tahmin etmemiz gerekiyor \n",
        "**Müşterilerimizin yaşam boyu değerlerini tahmin eden basit bir makine öğrenimi modeli oluşturacağız.**\n",
        "\n",
        "##Yaşam Boyu Değer Tahmini\n",
        "\n",
        "*   Müşteri Yaşam Boyu Değer hesaplaması için uygun bir zaman dilimi tanımlayın\n",
        "*   Geleceği tahmin etmek ve yaratmak için kullanacağımız özellikleri belirleyin\n",
        "*   Makine öğrenimi modelini eğitmek için yaşam boyu değeri (YBD) hesapla\n",
        "*   Makine öğrenimi modelini oluşturun ve çalıştırın\n",
        "*   Modelin faydalı olup olmadığını kontrol edin\n",
        "\n",
        "\n",
        "Zaman aralığına karar vermek sektörünüze, iş modelinize, stratejinize ve daha fazlasına bağlıdır. Bazı endüstriler için, 1 yıl çok uzun bir dönem iken, diğerleri için çok kısadır. Örneğimizde 6 ay ile devam edeceğiz \n",
        "\n",
        "\n",
        "Her bir müşteri kimliği için **RFM puanları** özellik kümesi için mükemmel adaylardır. Doğru bir şekilde uygulamak için veri kümemizi ayırmamız gerekir. 3 aylık veri alacağız, RFM'yi hesaplayacağız ve önümüzdeki 6 ayı tahmin etmek için kullanacağız. Bu yüzden önce iki veri çerçevesi oluşturmalı ve onlara RFM puanları eklemeliyiz."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lVEmAelNBdiU",
        "colab_type": "text"
      },
      "source": [
        "# Create 3m and 6m dataframes"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_UFB9N9wBdiW",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "tx_3m = tx_uk[(tx_uk.InvoiceDate < datetime(2011,6,1)) & (tx_uk.InvoiceDate >= datetime(2011,3,1))].reset_index(drop=True)\n",
        "tx_3m.head()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "F_H-gHP8Bdii",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "tx_6m = tx_uk[(tx_uk.InvoiceDate >= datetime(2011,6,1)) & (tx_uk.InvoiceDate < datetime(2011,12,1))].reset_index(drop=True)\n",
        "tx_6m.head()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NXyI5vFwBdir",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "tx_user = pd.DataFrame(tx_3m['CustomerID'].unique())\n",
        "tx_user.columns = ['CustomerID']\n",
        "tx_user.head()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jU3eiXY0AjHp",
        "colab_type": "text"
      },
      "source": [
        "#Cluster Method"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lW-LP_rzBdi1",
        "colab_type": "text"
      },
      "source": [
        "# Order cluster method"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ic8CWtXKBdi3",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def order_cluster(cluster_field_name, target_field_name,df,ascending):\n",
        "    new_cluster_field_name = 'new_' + cluster_field_name\n",
        "    df_new = df.groupby(cluster_field_name)[target_field_name].mean().reset_index()\n",
        "    df_new = df_new.sort_values(by=target_field_name,ascending=ascending).reset_index(drop=True)\n",
        "    df_new['index'] = df_new.index\n",
        "    df_final = pd.merge(df,df_new[[cluster_field_name,'index']], on=cluster_field_name)\n",
        "    df_final = df_final.drop([cluster_field_name],axis=1)\n",
        "    df_final = df_final.rename(columns={\"index\":cluster_field_name})\n",
        "    return df_final"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CZ_FjkLzBdjB",
        "colab_type": "text"
      },
      "source": [
        "# calculate recency score"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BBVvH56nBdjD",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "tx_max_purchase = tx_3m.groupby('CustomerID').InvoiceDate.max().reset_index()\n",
        "tx_max_purchase.columns = ['CustomerID','MaxPurchaseDate']\n",
        "tx_max_purchase['Recency'] = (tx_max_purchase['MaxPurchaseDate'].max() - tx_max_purchase['MaxPurchaseDate']).dt.days\n",
        "tx_user = pd.merge(tx_user, tx_max_purchase[['CustomerID','Recency']], on='CustomerID')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Y3nkvg6-BdjT",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "kmeans = KMeans(n_clusters=4)\n",
        "kmeans.fit(tx_user[['Recency']])\n",
        "tx_user['RecencyCluster'] = kmeans.predict(tx_user[['Recency']])\n",
        "\n",
        "tx_user = order_cluster('RecencyCluster', 'Recency',tx_user,False)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Dg9SDCtMBdja",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "tx_user.head()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-0G-Xh-Ws1ec",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "plt.scatter(tx_user['Recency'],tx_user['RecencyCluster'],c=tx_user['CustomerID'],s=200,cmap='Blues')\n",
        "plt.xlabel('Recency')\n",
        "plt.ylabel('Cluster')\n",
        "F = plt.gcf()\n",
        "Size = F.get_size_inches()\n",
        "F.set_size_inches(Size[0]*2, Size[1]*2, forward=True) # Set forward to True to resize window along with plot in figure.\n",
        "plt.show()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HjUK_EcpBdjn",
        "colab_type": "text"
      },
      "source": [
        "# calcuate frequency score"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "B-1fhcWhBdjp",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "tx_frequency = tx_3m.groupby('CustomerID').InvoiceDate.count().reset_index()\n",
        "tx_frequency.columns = ['CustomerID','Frequency']\n",
        "tx_user = pd.merge(tx_user, tx_frequency, on='CustomerID')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gXqAo5k9Bdj4",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "kmeans = KMeans(n_clusters=4)\n",
        "kmeans.fit(tx_user[['Frequency']])\n",
        "tx_user['FrequencyCluster'] = kmeans.predict(tx_user[['Frequency']])\n",
        "tx_user = order_cluster('FrequencyCluster', 'Frequency',tx_user,True)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NVkczuYVBdj8",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "tx_user.head()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aEhdsqsW2Eo-",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "plt.scatter(tx_user['Frequency'],tx_user['FrequencyCluster'],c=tx_user['CustomerID'],s=200,cmap='Blues')\n",
        "plt.xlabel('Frequency')\n",
        "plt.ylabel('FrequencyCluster')\n",
        "F = plt.gcf()\n",
        "Size = F.get_size_inches()\n",
        "F.set_size_inches(Size[0]*2, Size[1]*2, forward=True) # Set forward to True to resize window along with plot in figure.\n",
        "plt.show()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "h3QFRNZQBdkG",
        "colab_type": "text"
      },
      "source": [
        "## calcuate revenue score\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GyJSLpwtBdkH",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "tx_3m['Revenue'] = tx_3m['UnitPrice'] * tx_3m['Quantity']\n",
        "tx_revenue = tx_3m.groupby('CustomerID').Revenue.sum().reset_index()\n",
        "tx_user = pd.merge(tx_user, tx_revenue, on='CustomerID')\n",
        "\n",
        "kmeans = KMeans(n_clusters=4)\n",
        "kmeans.fit(tx_user[['Revenue']])\n",
        "tx_user['RevenueCluster'] = kmeans.predict(tx_user[['Revenue']])\n",
        "tx_user = order_cluster('RevenueCluster', 'Revenue',tx_user,True)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aptXDlQb3VGm",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "plt.scatter(tx_user['Revenue'],tx_user['RevenueCluster'],c=tx_user['CustomerID'],s=200,cmap='Blues')\n",
        "plt.xlabel('Revenue')\n",
        "plt.ylabel('RevenueCluster')\n",
        "F = plt.gcf()\n",
        "Size = F.get_size_inches()\n",
        "F.set_size_inches(Size[0]*2, Size[1]*2, forward=True) # Set forward to True to resize window along with plot in figure.\n",
        "plt.show()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1GjW4B_bBdkN",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "tx_user['OverallScore'] = tx_user['RecencyCluster'] + tx_user['FrequencyCluster'] + tx_user['RevenueCluster']\n",
        "tx_user['Segment'] = 'Low-Value'\n",
        "tx_user.loc[tx_user['OverallScore']>2,'Segment'] = 'Mid-Value' \n",
        "tx_user.loc[tx_user['OverallScore']>4,'Segment'] = 'High-Value' "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hKqDOHlbBdkU",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "tx_user"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_22S2oUo3-ZT",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "plt.scatter(tx_user['Segment'],tx_user['CustomerID'],c=tx_user['OverallScore'],s=300,cmap='Blues')\n",
        "plt.xlabel('OverallScore')\n",
        "plt.ylabel('CustomerID')\n",
        "F = plt.gcf()\n",
        "Size = F.get_size_inches()\n",
        "F.set_size_inches(Size[0]*2, Size[1]*2, forward=True) # Set forward to True to resize window along with plot in figure.\n",
        "plt.show()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "f2kjUMKGBdkZ",
        "colab_type": "text"
      },
      "source": [
        "## calculate revenue and create a new dataframe for it"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xHQf6YG5Bdkb",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "tx_6m['Revenue'] = tx_6m['UnitPrice'] * tx_6m['Quantity']\n",
        "tx_user_6m = tx_6m.groupby('CustomerID')['Revenue'].sum().reset_index()\n",
        "tx_user_6m.columns = ['CustomerID','m6_Revenue']"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WHfHijoUDp_U",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "plt.scatter(tx_user_6m['CustomerID'],tx_user_6m['m6_Revenue'],s=200,cmap='Blues')\n",
        "plt.xlabel('CustomerID')\n",
        "plt.ylabel('m6_Revenue')\n",
        "F = plt.gcf()\n",
        "Size = F.get_size_inches()\n",
        "F.set_size_inches(Size[0]*2, Size[1]*2, forward=True) # Set forward to True to resize window along with plot in figure.\n",
        "plt.show()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vEcZlvKwag8X",
        "colab_type": "text"
      },
      "source": [
        "![alt text](https://miro.medium.com/max/1400/1*3w0sJSAFCRQjhGKSG7ICYg.png)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wBJMkWTvBdks",
        "colab_type": "text"
      },
      "source": [
        "### Histogram clearly shows we have customers with negative LTV. We have some outliers too. Filtering out the outliers makes sense to have a proper machine learning model."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FGBIYRN9Bdku",
        "colab_type": "text"
      },
      "source": [
        "# We will merge our 3 months and 6 months dataframes to see correlations between LTV and the feature set we have."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7TA2hOQ0Bdkw",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "tx_merge = pd.merge(tx_user, tx_user_6m, on='CustomerID', how='left')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kaPCe49BBdk3",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "tx_merge.head()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8j5z1iuxBdk6",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "tx_graph = tx_merge.query(\"m6_Revenue < 30000\")"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6t6bh8vSBdk_",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "tx_graph.head()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xWsVEGfOcBIa",
        "colab_type": "text"
      },
      "source": [
        "özellik setimizi ve LTV verilerimizi birleştirir ve LTV ile genel RFM skorunu çizer"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wE-Y_6K2aTpJ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "plt.scatter(tx_graph['OverallScore'],tx_graph['m6_Revenue'], c=\"g\",s=200)\n",
        "plt.xlabel('OverallScore')\n",
        "plt.ylabel('m6_Revenue')\n",
        "F = plt.gcf()\n",
        "Size = F.get_size_inches()\n",
        "F.set_size_inches(Size[0]*2, Size[1]*2, forward=True) # Set forward to True to resize window along with plot in figure.\n",
        "plt.legend(loc='upper left')\n",
        "plt.show()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Dkb1Vlu2bwWo",
        "colab_type": "text"
      },
      "source": [
        "![alt text](https://miro.medium.com/max/1400/1*TTdNY2WV4n2yr_0IGD8bJA.png)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "smbv_R_eBdlH",
        "colab_type": "text"
      },
      "source": [
        "# The code below merges our feature set and LTV data and plots LTV vs overall RFM score:"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fyFfl-wacMXP",
        "colab_type": "text"
      },
      "source": [
        "Pozitif korelasyon burada oldukça görünür. Yüksek RFM puanı yüksek LTV anlamına gelir.\n",
        "\n",
        "Makine öğrenimi modelini oluşturmadan önce, bu makine öğrenimi sorununun türünü belirlememiz gerekir. LTV'nin kendisi bir gerileme problemidir. Bir makine öğrenimi modeli LTV'nin $ değerini tahmin edebilir. Ancak burada LTV segmentleri istiyoruz. Çünkü diğer insanlarla iletişim kurmayı daha eyleme geçirilebilir ve kolay hale getirir. K-araçları kümelemesi uygulayarak, mevcut LTV gruplarımızı belirleyebilir ve bunun üzerine segmentler oluşturabiliriz."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6MXIMxxZBdlP",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#remove outliers\n",
        "tx_merge = tx_merge[tx_merge['m6_Revenue']<tx_merge['m6_Revenue'].quantile(0.99)]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0Y02CPKnBdlU",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "tx_merge.head()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "j2VblQzM-9VB",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "wcss = {}\n",
        "for k in range(1,11):#30 adet orenklerememiz oldugu icin 1'den 30'a kadar gitmesini sagliyoruz\n",
        "    kmeans = KMeans(n_clusters= k, init= 'k-means++', max_iter= 300)\n",
        "    kmeans.fit(tx_merge[['m6_Revenue']])# standartlastirdigimi datamiza gore fit et, yani modelimizi olustur.\n",
        "    wcss[k] = kmeans.inertia_#inertia ne demek ? her bir k degeri icin wcss degerini bul  \n",
        "sns.pointplot(x = list(wcss.keys()), y = list(wcss.values()))\n",
        "wcss"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BKPluQkmeZWL",
        "colab_type": "text"
      },
      "source": [
        "**Before building the machine learning model, we need to identify what is the type of this machine learning problem. LTV itself is a regression problem. A machine learning model can predict the $ value of the LTV. But here, we want LTV segments. Because it makes it more actionable and easy to communicate with, other people. By applying K-means clustering,**\n",
        "\n",
        "**we can identify our existing LTV groups and build segments on top of it.Considering business part of this analysis, we need to treat customers differently based on their predicted LTV. For this example, we will apply clustering and have 3 segments**\n",
        "\n",
        "`(number of segments really depends on your business dynamics and goals):`\n",
        "\n",
        "\n",
        "```\n",
        "*   Low LTV\n",
        "*   Mid LTV\n",
        "*   High LTV\n",
        "\n",
        "```\n",
        "\n",
        "\n",
        "\n",
        "**We are going to apply K-means clustering to decide segments and observe their characteristics:**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TGRFIQlNBdlZ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "kmeans = KMeans(n_clusters=3)\n",
        "kmeans.fit(tx_merge[['m6_Revenue']])\n",
        "tx_merge['LTVCluster'] = kmeans.predict(tx_merge[['m6_Revenue']])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "140-TbJeBdld",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "tx_merge.head()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "w7bp9-oPBdlg",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#order cluster number based on LTV\n",
        "tx_merge = order_cluster('LTVCluster', 'm6_Revenue',tx_merge,True)\n",
        "tx_merge.head()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nTjrjsjX-Niq",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "plt.scatter(tx_merge['m6_Revenue'],tx_merge['LTVCluster'],c=tx_merge['CustomerID'],s=200,cmap='Blues')\n",
        "plt.xlabel('LTVCluster')\n",
        "plt.ylabel('m6_Revenue')\n",
        "F = plt.gcf()\n",
        "Size = F.get_size_inches()\n",
        "F.set_size_inches(Size[0]*2, Size[1]*2, forward=True) # Set forward to True to resize window along with plot in figure.\n",
        "plt.legend(loc='upper left')\n",
        "plt.show()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "A5bdGy91Bdlm",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#creatinga new cluster dataframe\n",
        "tx_cluster = tx_merge.copy()\n",
        "#see details of the clusters\n",
        "tx_cluster.groupby('LTVCluster')['m6_Revenue'].describe()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "R1-cXSDGz7jW",
        "colab_type": "text"
      },
      "source": [
        "### We have finished LTV clustering and here are the characteristics of each clusters\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "t_uDXo5-_G3v",
        "colab_type": "text"
      },
      "source": [
        "##Makine öğrenimi modelini eğitmeden önce birkaç adım daha var:\n",
        "\n",
        "\n",
        "* Bazı özellik mühendisliği yapmak gerekiyor. Kategorik sütunları sayısal sütunlara dönüştürmeliyiz.\n",
        "\n",
        "* Özelliklerin etiketimiz olan LTV kümeleriyle korelasyonunu kontrol edeceğiz.\n",
        "\n",
        "*  Özellik setimizi ve etiketimizi (LTV) X ve y olarak böleriz. X'i y'yi tahmin etmek için kullanırız.\n",
        "\n",
        "*  Eğitim ve Test veri kümesi oluşturur. Makine öğrenme modelinin oluşturulması için eğitim seti kullanılacaktır. Modelimizi gerçek performansını görmek için Test setine uygulayacağız.\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bSisY8eg_vBq",
        "colab_type": "text"
      },
      "source": [
        "İlk satırdan başlayalım. get_dummies () yöntemi, kategorik sütunları 0–1 gösterime dönüştürür. Örnekle tam olarak ne yaptığını görün:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "O254nBNx0DoQ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#convert categorical columns to numerical\n",
        "tx_class = pd.get_dummies(tx_cluster)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "n1xvMp_0IrOt",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "tx_merge.head()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "23MgoAxdATI1",
        "colab_type": "text"
      },
      "source": [
        "Bu get_dummies () öncesindeki veri setimizdi. Segment olan bir kategorik sütunumuz var. Get_dummies () uygulandıktan sonra ne olur:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "J0vfKtUd09_w",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "tx_class.head()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DPWcYFPrAula",
        "colab_type": "text"
      },
      "source": [
        "Segment sütunu gitti, ancak onu temsil eden yeni sayısal sütunlarımız var. 0 ve 1 ile 3 farklı sütuna dönüştürdük ve makine öğrenme modelimiz için kullanılabilir hale getirdik."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_moQ8tHz8iGW",
        "colab_type": "text"
      },
      "source": [
        "###calculate and show correlations\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IFOqwar48lha",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "corr_matrix = tx_class.corr()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EVsTiWjJ8uYh",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "corr_matrix #gorsel yap "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "avWYON2p87IB",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "corr_matrix['LTVCluster'].sort_values(ascending=False)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "k402-_7G_ggb",
        "colab_type": "text"
      },
      "source": [
        "###create X and y, X will be feature set and y is the label - LTV"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dLneBPxp9GDe",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "X = tx_class.drop(['LTVCluster','m6_Revenue'],axis=1)\n",
        "y = tx_class['LTVCluster']"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "m-b1nRxf_r4I",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "X.head()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TiQOhyQz_3zg",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "y.head()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "k5xGoReWD9vx",
        "colab_type": "text"
      },
      "source": [
        "###split training and test sets\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dxYRyHycDz5Z",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.25, random_state=56)\n",
        "print(X_train.shape, y_train.shape)\n",
        "print(X_test.shape, y_test.shape)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "e438cIPyEB4P",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "X_train.head()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9Tx7GLtwEPDO",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "X_test.head()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Rgr_Kn0jEY1W",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        " y_train.head()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "NJYCUHOBEdJB",
        "colab": {}
      },
      "source": [
        "y_test.head()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "W9fcAnRIFmrR",
        "colab_type": "text"
      },
      "source": [
        "Let’s start with the first line. get_dummies() method converts categorical columns to 0–1 notations. See what it exactly does with the example:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NVqc8Nf2EnrY",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "tx_cluster.head()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "seE3LOCHHnDh",
        "colab_type": "text"
      },
      "source": [
        "This was our dataset before get_dummies(). We have one categorical column which is Segment. What happens after applying get_dummies():"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_uj6ivykHr2b",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "tx_class.head()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fHKD4RmoXUc6",
        "colab_type": "text"
      },
      "source": [
        "Segment column is gone but we have new numerical ones which represent it. We have converted it to 3 different columns with 0 and 1 and made it usable for our machine learning model."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TFCmtxQmhtu-",
        "colab_type": "text"
      },
      "source": [
        "##Ensemble Methods\n",
        "\n",
        "Another way that we can improve the performance of algorithms on this problem is by using ensemble methods. In this section we will evaluate four diﬀerent ensemble machine learning algorithms, two boosting and two bagging methods:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qRzqOwg4hHxi",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Compare Algorithms\n",
        "import pandas\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn import model_selection\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.tree import DecisionTreeClassifier\n",
        "from sklearn.neighbors import KNeighborsClassifier\n",
        "from sklearn.discriminant_analysis import LinearDiscriminantAnalysis\n",
        "from sklearn.naive_bayes import GaussianNB\n",
        "from sklearn.svm import SVC\n",
        "# prepare models\n",
        "models = []\n",
        "models.append(('LR', LogisticRegression()))\n",
        "models.append(('LDA', LinearDiscriminantAnalysis()))\n",
        "models.append(('KNN', KNeighborsClassifier()))\n",
        "models.append(('DTree', DecisionTreeClassifier()))\n",
        "models.append(('NB', GaussianNB()))\n",
        "models.append(('SVM', SVC()))\n",
        "# evaluate each model in turn\n",
        "results = []\n",
        "names = []\n",
        "for name, model in models:\n",
        "\tkfold = model_selection.KFold(n_splits=2)\n",
        "\tcv_results = model_selection.cross_val_score(model,X,y, cv=kfold, scoring='accuracy')\n",
        "\tresults.append(cv_results)\n",
        "\tnames.append(name)\n",
        "\tmsg = \"%s: %f (%f)\" % (name, cv_results.mean(), cv_results.std())\n",
        "\tprint(msg)\n",
        "# boxplot algorithm comparison\n",
        "fig = plt.figure()\n",
        "fig.suptitle('Algorithm Comparison')\n",
        "ax = fig.add_subplot(111)\n",
        "plt.boxplot(results)\n",
        "ax.set_xticklabels(names)\n",
        "plt.show()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oEfsTdTMNMz_",
        "colab_type": "text"
      },
      "source": [
        "#Feature Scaling"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TxMAp551LtGa",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from sklearn.preprocessing import StandardScaler\n",
        "sc = StandardScaler()\n",
        "X_train = sc.fit_transform(X_train)\n",
        "X_test = sc.transform(X_test)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dqWsQfB4S8wq",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PPtsXV9LYFLf",
        "colab_type": "text"
      },
      "source": [
        "#Gaussian Naive Bayes classifier\n",
        "\n",
        "**Gaussian NB is based on the Naive Bayes theorem with the assumption of conditional independence between every pair of features given the label of the target class. The Graph for the likelihood of the feature vectors is Gaussian.**\n",
        "\n",
        "![Gaussian Naive Bayes classifier](https://www.codespeedy.com/wp-content/uploads/2019/12/iris_gaussian.png)\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rtJNZppXseFB",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from sklearn.naive_bayes import GaussianNB\n",
        "from sklearn.metrics import confusion_matrix, accuracy_score\n",
        "classifier = GaussianNB()\n",
        "classifier.fit(X_train, y_train)\n",
        "y_pred = classifier.predict(X_test)\n",
        "print(np.concatenate((y_pred.reshape(len(y_pred),1),y_test.values.reshape(len(y_test),1)),1))\n",
        "cm = confusion_matrix(y_test, y_pred)\n",
        "print(cm)\n",
        "print(accuracy_score(y_test, y_pred))\n",
        "print(classification_report(y_test, y_pred))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Re0QjR2wx3m0",
        "colab_type": "text"
      },
      "source": [
        "#Logistic Regression \n",
        "\n",
        "**Logistic Regression is one of the basic and powerful classifiers used in the machine learning model used for binary as well as multiclass classification problems. Let us apply logistic regression in the same way as we have applied the GaussianNB on the Iris dataset that we have and will be printing the accuracy score for this model as well.**\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WzfIvoG2x3Pb",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.metrics import mean_squared_error, r2_score\n",
        "classifier = LogisticRegression(solver = 'lbfgs',multi_class='auto')\n",
        "classifier.fit(X_train, y_train)\n",
        "y_pred = classifier.predict(X_test)\n",
        "print(np.concatenate((y_pred.reshape(len(y_pred),1),y_test.values.reshape(len(y_test),1)),1))\n",
        "cm = confusion_matrix(y_test, y_pred)\n",
        "print(cm)\n",
        "print(accuracy_score(y_test, y_pred))\n",
        "print(classification_report(y_test, y_pred))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gDfzyrWZ5svr",
        "colab_type": "text"
      },
      "source": [
        "#Decision tree classifier \n",
        "**Decision Tree classifier is a widely used classification technique where several conditions are put on the dataset in a hierarchical manner until the data corresponding to the labels is purely separated. **\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "v1-AY-pc20r1",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from sklearn.tree import DecisionTreeClassifier\n",
        "classifier = DecisionTreeClassifier()\n",
        "classifier.fit(X_train, y_train)\n",
        "y_pred = classifier.predict(X_test)\n",
        "print(np.concatenate((y_pred.reshape(len(y_pred),1),y_test.values.reshape(len(y_test),1)),1))\n",
        "cm = confusion_matrix(y_test, y_pred)\n",
        "print(cm)\n",
        "print(accuracy_score(y_test, y_pred))\n",
        "print(classification_report(y_test, y_pred))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LnOo1_ga9SQM",
        "colab_type": "text"
      },
      "source": [
        "#KNN (k-nearest neighbors) classifier \n",
        "\n",
        " KNN classifier is a very simple technique for classification and it is based upon the Euclidean distance between two data points calculated by taking the distance between the feature vector.\n",
        "In case of the same distance between a data point and data points belonging to two or more different classes then, the next lowest distance is calculated and it is assumed that the data point will belong to that class. The formula to calculate Euclidean distance between two data points is:\n",
        "\n",
        "![](https://www.codespeedy.com/wp-content/uploads/2019/12/iris_euclidean.png)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LBoe28LV-uEC",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from sklearn.neighbors import KNeighborsClassifier\n",
        "classifier = KNeighborsClassifier(n_neighbors=3,algorithm='ball_tree')\n",
        "classifier.fit(X_train, y_train)\n",
        "y_pred = classifier.predict(X_test)\n",
        "print(np.concatenate((y_pred.reshape(len(y_pred),1),y_test.values.reshape(len(y_test),1)),1))\n",
        "cm = confusion_matrix(y_test, y_pred)\n",
        "print(cm)\n",
        "print(accuracy_score(y_test, y_pred))\n",
        "print(classification_report(y_test, y_pred))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KLgG0gNE-_yf",
        "colab_type": "text"
      },
      "source": [
        "#Support Vector Machine\n",
        "Support Vector Machine or SVM is a classifier that classifies the data points into the classes(Supervised Learning) and separates those classes using a hyperplane.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3aHWeZ2Q_RAi",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from sklearn.svm import SVC\n",
        "classifier = SVC(C=50,kernel='rbf',gamma=1)     \n",
        "classifier.fit(X_train, y_train)\n",
        "y_pred = classifier.predict(X_test)\n",
        "print(np.concatenate((y_pred.reshape(len(y_pred),1),y_test.values.reshape(len(y_test),1)),1))\n",
        "cm = confusion_matrix(y_test, y_pred)\n",
        "print(cm)\n",
        "print(accuracy_score(y_test, y_pred))\n",
        "print(classification_report(y_test, y_pred))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RCOyLf2m_-aI",
        "colab_type": "text"
      },
      "source": [
        "# Random Forest Classifier \n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wWyIyS7D_yAt",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from sklearn.ensemble import RandomForestClassifier\n",
        "classifier = RandomForestClassifier(n_estimators = 10, criterion = 'entropy', random_state = 0)\n",
        "classifier.fit(X_train, y_train)\n",
        "y_pred = classifier.predict(X_test)\n",
        "print(np.concatenate((y_pred.reshape(len(y_pred),1),y_test.values.reshape(len(y_test),1)),1))\n",
        "cm = confusion_matrix(y_test, y_pred)\n",
        "print(cm)\n",
        "print(accuracy_score(y_test, y_pred))\n",
        "print(classification_report(y_test, y_pred))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cNCIdH0oRzt7",
        "colab_type": "text"
      },
      "source": [
        "#SVM Multiclassification Model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eyLga0GzRy1G",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from sklearn.svm import LinearSVC\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "scaler = StandardScaler()\n",
        "print(scaler.fit(X_train))\n",
        "StandardScaler()\n",
        "#print(scaler.mean_)\n",
        "print(scaler.transform(X_train))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8qte6w9kUvuj",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "clf = LinearSVC(random_state=0, tol=1e-5,max_iter=1200000,dual=False)\n",
        "svm_mul_class_model = clf.fit(X_train, y_train)\n",
        "svm_mul_class_model\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "R0ysS_mtV4ll",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "print('Accuracy of SVM classifier on training set: {:.2f}'\n",
        "       .format(svm_mul_class_model.score(X_train, y_train)))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sQsHQzqHWD6x",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "print('Accuracy of SVM classifier on test set: {:.2f}'\n",
        "       .format(svm_mul_class_model.score(X_test[X_train.columns], y_test)))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wU3sWSJAo3WQ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "\n",
        "# Transforming non numerical labels into numerical labels\n",
        "from sklearn import preprocessing\n",
        "encoder = preprocessing.LabelEncoder()\n",
        "# Dimension of Train and Test set \n",
        "\n",
        "# encoding train labels \n",
        "encoder.fit(y_train)\n",
        "y_train = encoder.transform(y_train)\n",
        "\n",
        "# encoding test labels \n",
        "encoder.fit(y_test)\n",
        "y_test = encoder.transform(y_test)\n",
        "names_of_predictors = list(X_train.columns.values)\n",
        "\n",
        "# Scaling the Train and Test feature set \n",
        "from sklearn.preprocessing import StandardScaler\n",
        "scaler = StandardScaler()\n",
        "X_train_scaled = scaler.fit_transform(X_train)\n",
        "X_test_scaled = scaler.transform(X_test)\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2T9gRiQt2Pit",
        "colab_type": "text"
      },
      "source": [
        "#Hyperparameter tuning using grid search and cross validation\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "q-ha18Dj2RL6",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#Libraries to Build Ensemble Model : Random Forest Classifier \n",
        "# Create the parameter grid based on the results of random search \n",
        "params_grid = [{'kernel': ['rbf'], 'gamma': [1e-3, 1e-4],\n",
        "                     'C': [1, 10, 100, 1000]},\n",
        "                    {'kernel': ['linear'], 'C': [1, 10, 100, 1000]}]                    "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zFHc0A8T3PB7",
        "colab_type": "text"
      },
      "source": [
        "#Training SVM model using radial kernel"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HCYjvbfp3Juy",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Performing CV to tune parameters for best SVM fit \n",
        "from sklearn.model_selection import cross_val_score, GridSearchCV\n",
        "from sklearn.svm import SVC\n",
        "svm_model = GridSearchCV(SVC(), params_grid, cv=5)\n",
        "svm_model.fit(X_train_scaled, y_train)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1C3BlUJn_GeJ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "print(svm_model)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "W3-2_bIo_6VX",
        "colab_type": "text"
      },
      "source": [
        "#Confusion Matrix and Accuracy Score"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kZn4If9V__sh",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# View the accuracy score\n",
        "print('Best score for training data:', svm_model.best_score_,\"\\n\") "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DdkXx4m1_pLP",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# View the best parameters for the model found using grid search\n",
        "print('Best C:',svm_model.best_estimator_.C,\"\\n\") \n",
        "print('Best Kernel:',svm_model.best_estimator_.kernel,\"\\n\")\n",
        "print('Best Gamma:',svm_model.best_estimator_.gamma,\"\\n\")"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vnSwIfiOBKwQ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "final_model = svm_model.best_estimator_\n",
        "Y_pred = final_model.predict(X_test_scaled)\n",
        "Y_pred_label = list(encoder.inverse_transform(Y_pred))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IhFSfeUfBoz2",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Making the Confusion Matrix\n",
        "#print(pd.crosstab(Y_test_label, Y_pred_label, rownames=['Actual Activity'], colnames=['Predicted Activity']))\n",
        "print(confusion_matrix(y_test,Y_pred_label))\n",
        "print(\"\\n\")\n",
        "print(classification_report(y_test,Y_pred_label))\n",
        "\n",
        "print(\"Training set score for SVM: %f\" % final_model.score(X_train_scaled , y_train))\n",
        "print(\"Testing  set score for SVM: %f\" % final_model.score(X_test_scaled  , y_test ))\n",
        "\n",
        "svm_model.score"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BW4Xyo3aYHUl",
        "colab_type": "text"
      },
      "source": [
        "#XGBoost Multiclassification Model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7ut2jy4OYLuR",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "ltv_xgb_model = xgb.XGBClassifier(max_depth=5, learning_rate=0.1,objective= 'multi:softprob',n_jobs=-1).fit(X_train, y_train)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5yekR7MCcQ8Q",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "ltv_xgb_model"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IBKQ7rgncZuy",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "print('Accuracy of XGB classifier on training set: {:.2f}'\n",
        "       .format(ltv_xgb_model.score(X_train, y_train)))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7_PdXMpudlE7",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "print('Accuracy of XGB classifier on test set: {:.2f}'\n",
        "       .format(ltv_xgb_model.score(X_test[X_train.columns], y_test)))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TdJjvSU5e_uv",
        "colab_type": "text"
      },
      "source": [
        "##We can identify that by looking at classification report:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KXXndMKMdsgb",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "y_pred = ltv_xgb_model.predict(X_test)\n",
        "print(classification_report(y_test, y_pred))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "btamfgPPeKa-",
        "colab_type": "text"
      },
      "source": [
        "First we need to check our benchmark. Biggest cluster we have is cluster 0 which is 76.5% of the total base. If we blindly say, every customer belongs to cluster 0, then our accuracy would be 76.5%.\n",
        "\n",
        "84% vs 76.5% tell us that our machine learning model is a useful one but needs some improvement for sure. We should find out where the model is failing."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aZ3MG8FRkCOV",
        "colab_type": "text"
      },
      "source": [
        "Precision and recall are acceptable for 0. As an example, for cluster 0 (Low LTV), if model tells us this customer belongs to cluster 0, 90 out of 100 will be correct (precision). And the model successfully identifies 93% of actual cluster 0 customers (recall). We really need to improve the model for other clusters. For example, we barely detect 56% of Mid LTV customers. Possible actions to improve those points:\n",
        "\n",
        "    Adding more features and improve feature engineering\n",
        "    Try different models other than XGBoost\n",
        "    Apply hyper parameter tuning to current model\n",
        "    Add more data to the model if possible"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dp6iiUGMqSTm",
        "colab_type": "text"
      },
      "source": [
        "#Performance Comparison\n",
        "In the previous sections we have used the **accuracy_score()** method to measure the accuracy of the different algorithms. Now, we will use the `ClassificationReport` class provided by the **Yellowbrick** library to give us a visual report of how our models perform.\n",
        "\n",
        "#GaussianNB"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9ieqiUN7qRsl",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from yellowbrick.classifier import ClassificationReport\n",
        "# Instantiate the classification model and visualizer\n",
        "visualizer = ClassificationReport(gnb, classes=['Won','Loss'])\n",
        "visualizer.fit(X_train,y_train) # Fit the training data to the visualizer\n",
        "visualizer.score(X_test, y_test) # Evaluate the model on the test data\n",
        "g = visualizer.poof() # Draw/show/poof the data"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QzQpLzTbP543",
        "colab_type": "text"
      },
      "source": [
        "In the code above, first we import the ClassificationReport class provided by the yellowbrick.classifier module. Next, an object visualizer of the type ClassificationReport is created. Here the first argument is the GaussianNB object gnb that was created while implementing the Naive-Bayes algorithm in the ‘Naive-Bayes’ section. The second argument contains the labels ‘Won’ and ‘Loss’ from the ‘Opportunity Result’ column from the sales_data dataframe.\n",
        "\n",
        "Next, we use the fit() method to train the visualizer object. This is followed by the score() method, which uses gnb object to carry out predictions as per the GaussianNB algorithm and then calculate the accuracy score of the predictions made by this algorithm. Finally, we use the poof() method to draw a plot of the different scores for the GaussianNB algorithm. Notice how the different scores are laid out against each of the labels ‘Won’ and ‘Loss’; this enables us to visualize the scores across the different target classes.\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gXTP7b1syZuD",
        "colab_type": "text"
      },
      "source": [
        "#LinearSVC"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "i-MP9_oMRH2q",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from yellowbrick.classifier import ClassificationReport\n",
        "# Instantiate the classification model and visualizer\n",
        "visualizer = ClassificationReport(svm_model, classes=['Won','Loss'])\n",
        "visualizer.fit(X_train,y_train) # Fit the training data to the visualizer\n",
        "visualizer.score(X_test, y_test) # Evaluate the model on the test data\n",
        "g = visualizer.poof() # Draw/show/poof the data"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "W7YJsqYhvT80",
        "colab_type": "text"
      },
      "source": [
        "In the code above, first we imported the ClassificationReport class provided by the yellowbrick.classifier module. Next, an object visualizer of the type ClassificationReport was created. Here the first argument is the LinearSVC object svc_model, that was created while implementing the LinearSVC algorithm in the ‘LinearSVC’ section. The second argument contains the labels ‘Won’ and ‘Loss’ from the ‘Opportunity Result’ column from the sales_data dataframe.\n",
        "\n",
        "Next, we used the fit() method to train the ‘svc_model’ object. This is followed by the score() method which uses the svc_model object to carry out predictions according to the LinearSVC algorithm and then calculate the accuracy score of the predictions made by this algorithm. Finally, we used the poof() method to draw a plot of the different scores for the LinearSVC algorithm."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iQ3WEwgEmoXT",
        "colab_type": "text"
      },
      "source": [
        "#Decision tree classifier\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "k-7AU1JsyBzK",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "DT"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "b_4oK3LKvdtC",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from yellowbrick.classifier import ClassificationReport\n",
        "# Instantiate the classification model and visualizer\n",
        "visualizer = ClassificationReport(dt, classes=['Won','Loss'])\n",
        "visualizer.fit(X_train,y_train) # Fit the training data to the visualizer\n",
        "visualizer.score(X_test, y_test) # Evaluate the model on the test data\n",
        "g = visualizer.poof() # Draw/show/poof the data"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "u7DdNg8WyTaA",
        "colab_type": "text"
      },
      "source": [
        "#svm Multclass"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6pgWg5IuySrj",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from yellowbrick.classifier import ClassificationReport\n",
        "# Instantiate the classification model and visualizer\n",
        "visualizer = ClassificationReport(clf, classes=['Won','Loss'])\n",
        "visualizer.fit(X_train,y_train) # Fit the training data to the visualizer\n",
        "visualizer.score(X_test, y_test) # Evaluate the model on the test data\n",
        "g = visualizer.poof() # Draw/show/poof the data"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5xSgeRRry3RL",
        "colab_type": "text"
      },
      "source": [
        "#Random Forest Classifier"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "O1DIHwk_y4rq",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "classifier"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "K38XEOAhzEqP",
        "colab": {}
      },
      "source": [
        "from yellowbrick.classifier import ClassificationReport\n",
        "# Instantiate the classification model and visualizer\n",
        "visualizer = ClassificationReport(classifier, classes=['Won','Loss'])\n",
        "visualizer.fit(X_train,y_train) # Fit the training data to the visualizer\n",
        "visualizer.score(X_test, y_test) # Evaluate the model on the test data\n",
        "g = visualizer.poof() # Draw/show/poof the data"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OKPyd5xNzKIy",
        "colab_type": "text"
      },
      "source": [
        "#Logistic Regression"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bHlVYld3zKnV",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "logreg"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oJ0gvUxDzXbj",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from yellowbrick.classifier import ClassificationReport\n",
        "# Instantiate the classification model and visualizer\n",
        "visualizer = ClassificationReport(logreg, classes=['Won','Loss'])\n",
        "visualizer.fit(X_train,y_train) # Fit the training data to the visualizer\n",
        "visualizer.score(X_test, y_test) # Evaluate the model on the test data\n",
        "g = visualizer.poof() # Draw/show/poof the data"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "73t3vBDZ2k30",
        "colab_type": "text"
      },
      "source": [
        "#KNN (k-nearest neighbors) classifier"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WY5p8RdH2u0q",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "clfKN"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5lMrHZin2xBq",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from yellowbrick.classifier import ClassificationReport\n",
        "# Instantiate the classification model and visualizer\n",
        "visualizer = ClassificationReport(clfKN, classes=['Won','Loss'])\n",
        "visualizer.fit(X_train,y_train) # Fit the training data to the visualizer\n",
        "visualizer.score(X_test, y_test) # Evaluate the model on the test data\n",
        "g = visualizer.poof() # Draw/show/poof the data"
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}